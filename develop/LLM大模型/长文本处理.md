## 长文本处理

在自然语言处理（NLP）中，处理长文本（超出模型上下文窗口限制的文本）是一个重要挑战。目前主流的长文本处理技术可以分为以下几类：

------

## **1. 分块（Chunking）策略**

### **(1) 固定长度分块（Fixed-Length Chunking）**

- **方法**：将文本按固定长度（如512 tokens）分割，独立编码每块。
- **问题**：可能切断句子或段落，丢失上下文。
- **改进**：
  - **重叠分块（Overlapping Chunking）**：相邻块部分重叠（如50 tokens），减少边界信息丢失。
  - **句子/段落对齐分块**：确保分块在自然语言边界（如句末）切断。

### **(2) 动态分块（Dynamic Chunking）**

- **Late Chunking（迟分策略）**：先全局分析，再按语义分块（如基于TextTiling或BERT的注意力机制）。
- **语义分块（Semantic Chunking）**：使用聚类或主题模型（如LDA）划分相关段落。

------

## **2. 层次化编码（Hierarchical Encoding）**

### **(1) 句子/段落级编码**

- **方法**：
  1. 先对句子或段落单独编码（如用BERT）。
  2. 再使用RNN/Transformer聚合所有句子的表示。
- **代表模型**：
  - **Hierarchical LSTM/Transformer**（如DocBERT、Longformer）。
  - **Setence-BERT（SBERT）**：对句子编码后计算相似度。

### **(2) 递归/分层Transformer**

- **方法**：
  - 低层处理短片段，高层聚合片段表示。
- **代表模型**：
  - **Transformer-XL**：引入递归机制，缓存历史片段信息。
  - **BigBird**：使用稀疏注意力机制处理长文本。

------

## **3. 稀疏注意力（Sparse Attention）**

### **(1) 局部+全局注意力**

- **方法**：只计算部分token的注意力，减少计算量。
- **代表模型**：
  - **Longformer**：滑动窗口（局部）+全局注意力（关键token）。
  - **BigBird**：结合随机注意力、局部窗口和全局token。

### **(2) 线性注意力（Linear Attention）**

- **方法**：用近似计算降低注意力复杂度（从O(n²)到O(n)）。
- **代表模型**：
  - **Reformer**：基于LSH（局部敏感哈希）的注意力。
  - **Performer**：使用随机正交特征（FAVOR+）加速。

------

## **4. 记忆增强（Memory-Augmented）方法**

### **(1) 外部记忆库**

- **方法**：存储历史片段信息，供后续查询。
- **代表模型**：
  - **MemNN（Memory Networks）**：显式存储关键信息。
  - **RAG（Retrieval-Augmented Generation）**：检索增强生成。

### **(2) 缓存机制（Cache-Based）**

- **方法**：缓存前文隐藏状态，避免重复计算。
- **代表模型**：
  - **Transformer-XL**：缓存前片段的隐藏状态。
  - **Compressive Transformer**：压缩历史信息存储。

------

## **5. 检索增强（Retrieval-Augmented）方法**

### **(1) 检索+生成**

- **方法**：先检索相关段落，再生成答案。
- **代表模型**：
  - **REPLUG**：检索增强的LM微调。
  - **FiD（Fusion-in-Decoder）**：多文档检索后融合生成。

### **(2) 动态检索**

- **方法**：在生成过程中动态检索外部知识。
- **代表模型**：
  - **Atlas**：基于检索的少样本学习。

------

## **6. 压缩与摘要（Compression & Summarization）**

### **(1) 文本压缩**

- **方法**：删除冗余信息，保留关键内容。
- **技术**：
  - **抽取式摘要**（如TextRank）。
  - **生成式摘要**（如BART、PEGASUS）。

### **(2) 潜在表示压缩**

- **方法**：用低维向量表示长文本。
- **代表模型**：
  - **BPR（Bi-Encoder）**：双塔模型编码长文本。
  - **DPR（Dense Passage Retrieval）**：稠密检索。

------

## **7. 长文本专用模型**

| **模型**                              | **方法**                     | **最大长度** |
| :------------------------------------ | :--------------------------- | :----------- |
| **Longformer**                        | 稀疏注意力（滑动窗口+全局）  | 4K-16K       |
| **BigBird**                           | 稀疏注意力（随机+局部+全局） | 4K-16K       |
| **LED（Longformer-Encoder-Decoder）** | 适用于长文本生成             | 16K+         |
| **Reformer**                          | LSH注意力+分块               | 64K+         |
| **Performer**                         | 线性注意力（FAVOR+）         | 100K+        |

------

## **总结：如何选择合适的长文本处理技术？**

| **场景**                | **推荐方法**                         |
| :---------------------- | :----------------------------------- |
| 固定长度文档分类        | **分块+池化**（如BERT-MaxPool）      |
| 长文本问答/检索         | **检索增强（RAG/FiD）**              |
| 超长文本（>10K tokens） | **稀疏注意力（Longformer/BigBird）** |
| 低计算资源环境          | **层次化编码（Hierarchical LSTM）**  |
| 生成任务（如摘要）      | **压缩+生成（BART/PEGASUS）**        |

目前，**稀疏注意力模型（如Longformer、BigBird）和检索增强方法（如RAG）** 是工业界的主流选择，而**Late Chunking、层次化编码**在特定任务（如文档聚类）中仍具优势。未来，**线性注意力（Performer）和动态检索（Atlas）** 可能成为更高效的解决方案。